# news_summary.py

import requests
from bs4 import BeautifulSoup
from openai import OpenAI
from datetime import datetime
import os
import json
#from dotenv import load_dotenv

# ì „ì—­ ë””ë²„ê·¸ ì„¤ì •
DEBUG_MODE = False

headers = {'User-Agent': 'Mozilla/5.0'}

# .env ê²½ë¡œ ì„¤ì •
#load_dotenv(os.path.join(os.path.dirname(__file__), "..", ".env"))

def get_naver_news(query, max_results=3):
    url = f"https://search.naver.com/search.naver?where=news&query={query}"
    res = requests.get(url, headers=headers)

    soup = BeautifulSoup(res.text, 'html.parser')
    items = soup.select('a[href^="https://n.news.naver.com/"]')[:max_results]
    
    # ë””ë²„ê¹… ë‰´ìŠ¤ ê°¯ìˆ˜ ì¶œë ¥
    if DEBUG_MODE:
        print(f"[NAVER] ë‰´ìŠ¤ ê°œìˆ˜: {len(items)}")
    return [(a.text.strip(), a['href']) for a in items]

def get_google_news(query, max_results=3):
    url = f"https://news.google.com/search?q={query}&hl=ko&gl=KR&ceid=KR:ko"
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                      "AppleWebKit/537.36 (KHTML, like Gecko) "
                      "Chrome/113.0.0.0 Safari/537.36"
    }
    res = requests.get(url, headers=headers)

    soup = BeautifulSoup(res.text, "html.parser")
    items = soup.select("a.JtKRv")[:max_results]
    news_list = []
    for a in items:
        title = a.text.strip()
        href = a.get("href", "")
        # ìƒëŒ€ê²½ë¡œë¥¼ ì ˆëŒ€ê²½ë¡œë¡œ ë³€ê²½
        if href.startswith("./"):
            href = "https://news.google.com" + href[1:]
        news_list.append((title, href))
    if DEBUG_MODE:
        print(f"[Google] ë‰´ìŠ¤ ê°œìˆ˜: {len(news_list)}")
    return news_list

def fetch_article_content(url):
    try:
        res = requests.get(url, headers=headers, timeout=5)
        soup = BeautifulSoup(res.text, 'html.parser')
        paragraphs = soup.select('article p')
        text = '\n'.join([p.text.strip() for p in paragraphs if p.text.strip()])
        return text[:1000]
    except Exception as e:
        return f"(ë³¸ë¬¸ ìˆ˜ì§‘ ì‹¤íŒ¨: {e})"

def fetch_naver_article_content(url):
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)',
            'Referer': 'https://www.naver.com'
        }
        res = requests.get(url, headers=headers, timeout=5)
        soup = BeautifulSoup(res.text, 'html.parser')
        article = soup.select_one('article')
        if article:
            paragraphs = article.find_all(['p', 'div'])
            text = '\n'.join([p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)])
            return text[:2000]
        else:
            return "(ê¸°ì‚¬ ë³¸ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ)"
    except Exception as e:
        return f"(ë³¸ë¬¸ ìˆ˜ì§‘ ì‹¤íŒ¨: {e})"

def summarize_with_gpt(title, url):
    client = OpenAI(api_key=os.getenv("OPENAI_API_KEY") or "YOUR KEY")
    if "n.news.naver.com" in url:
        content = fetch_naver_article_content(url)
        prompt = f"ë‹¤ìŒì€ ë‰´ìŠ¤ ê¸°ì‚¬ì…ë‹ˆë‹¤. ê¸°ì‚¬ì˜ ì œëª©ì„ ì²« ì¤„ì— í¬í•¨í•´ì£¼ì„¸ìš”. ê·¸ë¦¬ê³ , í•µì‹¬ ë‚´ìš©ì„ 3ì¤„ ì´ë‚´ë¡œ ìš”ì•½í•´ ì£¼ì„¸ìš”.\n\nì œëª©: {title}\në³¸ë¬¸:\n{content}"
    else:
        content = fetch_article_content(url)
        prompt = f"ë‹¤ìŒì€ ë‰´ìŠ¤ ê¸°ì‚¬ì…ë‹ˆë‹¤. í•µì‹¬ ë‚´ìš©ì„ 3ì¤„ ì´ë‚´ë¡œ ìš”ì•½í•´ ì£¼ì„¸ìš”:\n\nì œëª©: {title}\në‚´ìš©: {content}"
    try:
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": prompt}]
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        return f"(ìš”ì•½ ì‹¤íŒ¨: {e})"
    

API_KEY = os.getenv("GEMINI_API_KEY")
API_URL = "https://generativelanguage.googleapis.com/v1/models/gemini-2.5-flash:generateContent"

headers = {
    "Content-Type": "application/json"
}

def summarize_with_gemini(title: str, url: str) -> str:
    """ë‰´ìŠ¤ ì œëª©ê³¼ URLì„ ë°›ì•„ Geminië¥¼ ì‚¬ìš©í•´ ìš”ì•½ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    prompt = f"""
    ë‹¤ìŒì€ ë‰´ìŠ¤ ì œëª©ê³¼ URLì…ë‹ˆë‹¤. í•´ë‹¹ ë‰´ìŠ¤ì˜ ë‚´ìš©ì„ ì§§ê³  í•µì‹¬ì ìœ¼ë¡œ ìš”ì•½í•´ ì£¼ì„¸ìš”.
    
    ì œëª©: {title}
    ê¸°ì‚¬ ë§í¬: {url}

    ìš”ì•½ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•´ ì£¼ì„¸ìš”.
    """

    data = {
        "contents": [{
            "parts": [{
                "text": prompt.strip()
            }]
        }]
    }

    params = {
        "key": API_KEY
    }

    try:
        response = requests.post(API_URL, headers=headers, params=params, data=json.dumps(data))
        response.raise_for_status()
        result = response.json()
        summary_text = result["candidates"][0]["content"]["parts"][0]["text"]
        return summary_text.strip()
    except Exception as e:
        print(f"Gemini ìš”ì•½ ìš”ì²­ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return "ìš”ì•½ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."


def run_news_summary(query):
    #result = {"ë„¤ì´ë²„": [], "êµ¬ê¸€": []}
    #for portal, func in [("ë„¤ì´ë²„", get_naver_news), ("êµ¬ê¸€", get_google_news)]:
    result = {"êµ¬ê¸€": []}
    for portal, func in [("êµ¬ê¸€", get_google_news)]:

        try:
            for title, url in func(query):
                #summary = summarize_with_gpt(title, url)
                summary = summarize_with_gemini(title, url)
                result[portal].append((title, url, summary))
        except Exception as e:
            result[portal].append(("ìˆ˜ì§‘ ì‹¤íŒ¨", "#", str(e)))
    return result

def save_all_queries_to_markdown(all_results, all_queries, output_dir="/volume2/backups/Obsidian/Newbie/0. Inbox"):
    today = datetime.now().strftime('%Y-%m-%d')
    filename = os.path.join(output_dir, f"multi_news_ALL_{today}.md")
    with open(filename, 'w', encoding='utf-8') as f:
        f.write(f"# ğŸ“° {today} - ì „ì²´ í‚¤ì›Œë“œ ë‰´ìŠ¤ ìš”ì•½\n\n")
        for query in all_queries:
            f.write(f"\n---\n\n## ğŸ” í‚¤ì›Œë“œ: {query}\n\n")
            result = all_results[query]
            for portal, items in result.items():
                f.write(f"### ğŸ” {portal} ë‰´ìŠ¤\n")
                for title, url, summary in items:
                    f.write(f"- [{title}]({url})\n  > {summary}\n\n")
    return filename

if __name__ == "__main__":

    queries = ["AI ê¸°ìˆ "]

    all_results = {}
    for q in queries:
        print(f"â–¶ï¸ ìˆ˜ì§‘ ì‹œì‘: {q}")
        result = run_news_summary(q)
        all_results[q] = result

    #final_path = save_all_queries_to_markdown(all_results, queries)
    #print(f"âœ… ì „ì²´ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {final_path}")
    print(all_results)
